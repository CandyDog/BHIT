{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BHIT (TensorFlow)\n",
    "\n",
    "This notebook uses python and **TensorFlow** package to run BHIT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Packages\n",
    "\n",
    "1. `time` package for measuring running time of the whole program;\n",
    "2. `numpy` package for matrix manipulation;\n",
    "3. `tensorflow` package for GPU-accelerated calculation.\n",
    "4. `OrderedDict` class for constructing ordered selective functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Some Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function init()\n",
    "This function intializes parameters that will be used in the program.\n",
    "\n",
    "- `dataset`: The whole dataset read from the input file.\n",
    "- `outFileName`: Name of output file.\n",
    "- `iterNum`: Number of iteration.\n",
    "- `burninNum`: Burn-in number of iterations.\n",
    "- `ObsNum`: Number of observations, e.g., number of population.\n",
    "- `SNPNum`: Number of single nucleotide polymorphisms (SNPs).\n",
    "- `PhenoNum`: Number of phenotype types.\n",
    "- `MAF`: Minor Allele Frequency, should be less than 1.\n",
    "\n",
    "**There are some differences between Jupyter notebook version and command line version!**\n",
    "\n",
    "*We define parameter values inside the function instead of reading them from command line, because Jupyter cannot run with self-defined parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    \n",
    "    dataset = np.loadtxt('input.txt').transpose()\n",
    "    outFileName = 'output.txt'\n",
    "    iterNum = 30000\n",
    "    burninNum = 29000\n",
    "    ObsNum = 200\n",
    "    SNPNum = 100\n",
    "    PhenoNum = 1\n",
    "    MAF = 0.5\n",
    "    \n",
    "    return dataset, outFileName, iterNum, burninNum, ObsNum, SNPNum, PhenoNum, MAF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function logLikeCont(contArr)\n",
    "This function calculates logarithmic likelihood of only continuous variates given the data array.\n",
    "#### Math formulas\n",
    "We assume $Gaussian$ distribution on the continuous data, and the probability is:\n",
    "\n",
    "$$P(Y\\mid X) = (\\frac{1}{2\\pi})^n\\sqrt{\\frac{\\kappa_0}{\\kappa_n}}\\frac{\\Gamma(v_n/2)}{\\Gamma(v_0/2)}((\\frac{v_0\\sigma_0^2}{2})^{v_0/2}/(\\frac{v_n\\sigma_n^2}{2})^{v_n/2})$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\kappa_n = \\kappa_0 + n$\n",
    "- $v_n = v_0 + n$\n",
    "- $\\sigma_n^2 = \\frac{1}{v_n}(v_0\\sigma_0^2+(n-1)s^2+\\frac{\\kappa_0 n}{\\kappa_n}(\\bar y - \\mu_0)^2)$\n",
    "- $s^2 = \\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar y)^2$\n",
    "- $\\kappa_0, \\mu_0, \\sigma_0, v_0$ is pre-defined in the program ([click here](#Run-This-Program)), $n$ is the observation number of continuous data\n",
    "\n",
    "#### Parameters\n",
    "- `contTensor`: Input tensor.\n",
    "\n",
    "#### Returns\n",
    "- `logProb`: Logarithmic likelihood of continuous variates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikeCont(contTensor):\n",
    "    varNum = tf.shape(contTensor)[0]\n",
    "    obsNum = tf.cast(tf.shape(contTensor)[1], tf.float64)\n",
    "    \n",
    "    def func1():\n",
    "        means = tf.reduce_mean(contTensor)\n",
    "        sigma = tf.constant(1.0, dtype=tf.float64)\n",
    "        \n",
    "        nuVar = tf.multiply(NU0, tf.square(sigma))\n",
    "        nuVar += tf.reduce_sum(tf.square(contTensor-means))\n",
    "        nuVar += tf.multiply(KAPPA0, tf.multiply(\n",
    "            obsNum/(KAPPA0+obsNum), tf.square(means-MU0)))\n",
    "\n",
    "        res = (-1*tf.log(2*PI)*obsNum / 2 + tf.log(KAPPA0 / \n",
    "                (KAPPA0 + obsNum))/2 + tf.lgamma((NU0+obsNum)/2))\n",
    "        res += (-1*tf.lgamma(NU0/2) + tf.log(NU0*tf.square(sigma)/\n",
    "                2)*NU0/2 - tf.log(nuVar/2) * (NU0+obsNum)/2)\n",
    "        return res\n",
    "    \n",
    "    def func2():\n",
    "        means = tf.reduce_mean(contTensor, axis=1)\n",
    "        lambda_arr = tf.diag(tf.ones_like(contTensor)[0])\n",
    "        diff = tf.reshape(means-MU0, [1, -1])\n",
    "        lambdaN = (lambda_arr + KAPPA0*obsNum/(KAPPA0+obsNum) * tf.matmul(\n",
    "                    diff, diff, transpose_a=True, transpose_b=False))\n",
    "        lambdaN += (obsNum-1)*np.cov(contTensor, rowvar=False, bias=False)\n",
    " \n",
    "        logProb = (-tf.log(PI) * obsNum * varNum / 2 + tf.log(KAPPA0/\n",
    "            (KAPPA0 + obsNum) * varNum / 2))\n",
    "        logProb += tf.log(tf.matrix_determinant(lambda_arr)) * NU0/2\n",
    "        logProb -= tf.log(tf.matrix_determinant(lambdaN)) * (NU0+obsNum)/2\n",
    "        logProb += tf.reduce_sum(tf.lgamma((NU0+obsNum)/2 - \n",
    "            np.arange(varNum)/2) - tf.lgamma(NU0/2 - \n",
    "            np.arange(varNum)/2))\n",
    "    \n",
    "    logProb = tf.cond(tf.equal(varNum, 1), lambda: func1(), lambda: ZERO)\n",
    "        \n",
    "    return logProb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function logLikeDisc(discArr)\n",
    "This function calculates logarithmic likelihood of only discrete variates given the data array.\n",
    "#### Math formulas\n",
    "We assume $Dirichlet$ distribution on the discrete data, and the probability is:\n",
    "\n",
    "$$P(p_1, \\cdots, p_{C_h}\\mid \\alpha_1, \\cdots, \\alpha_{C_h}) = \\frac{1}{B(\\alpha)}\\prod_{j=1}^{C_h}p_{j}^{\\alpha_{j-1}} $$\n",
    "\n",
    "where \n",
    "\n",
    "- $C_h$ is the possible combination values in the genetic variation group (incoming parameter `discArr`)\n",
    "- $\\alpha = (\\alpha_1, \\cdots, \\alpha_{C_h})$, parameter vector in $Dirichlet$ distribution\n",
    "- $B(\\alpha) = \\frac{\\prod_{j=1}^{C_h}\\Gamma(\\alpha_j)}{\\Gamma(\\sum_{j=1}^{C_h}\\alpha_j)}$\n",
    "\n",
    "\n",
    "By integrating these, we can get following formula:\n",
    "$$ P(X\\mid I) = \\prod_{j=1}^{C_h}\\frac{\\Gamma(n_j+\\alpha_j)}{\\Gamma(\\alpha_j)}\\frac{\\Gamma(\\sum_{j=1}^{C_h}\\alpha_j)}{\\Gamma(\\sum_{j=1}^{C_h}(n_j+\\alpha_j))}$$\n",
    "\n",
    "where \n",
    "\n",
    "- $X$ is the genetic variation group, `discArr` in the program\n",
    "- $I$ is current partition, `Ix` or `Iy` in the program\n",
    "- $n_j$ denotes the number of j-th value in possible combination shown up in the genetic variation group $X$\n",
    "- $\\Gamma (x) = \\int_0^{\\infty}t^{x-1}e^{-t}dt$.\n",
    "\n",
    "#### Parameters:\n",
    "- `discTensor`: Input tensor.\n",
    "\n",
    "#### Returns\n",
    "- `logProb`: Logarithmic likelihood of discrete variates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikeDisc(discTensor):\n",
    "    logProb = tf.constant(0.0, dtype=tf.float64)\n",
    "    combined_tensor = tf.reduce_join(discTensor, 0, separator=' ')\n",
    "    unique_tensor, _, N = tf.unique_with_counts(combined_tensor)\n",
    "    \n",
    "    idx = tf.string_split(unique_tensor, delimiter=' ')\n",
    "    idx = tf.sparse_tensor_to_dense(idx, default_value='1')\n",
    "    idx = tf.string_to_number(idx, out_type=tf.int32) - 1\n",
    "    \n",
    "    alpha = tf.gather(Odds, idx)\n",
    "    alpha = tf.reduce_prod(alpha, axis=1)\n",
    "    n_plus_alpha = tf.add(alpha, tf.cast(N, alpha.dtype))\n",
    "    \n",
    "    logProb += tf.reduce_sum(tf.lgamma(n_plus_alpha) - tf.lgamma(alpha))\n",
    "    logProb -= tf.lgamma(tf.reduce_sum(n_plus_alpha))\n",
    "    return logProb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function logLikeDepe(discArr, contArr)\n",
    "This function calculates logarithmic likelihood of partitions with both continuous and discrete variates.\n",
    "#### Math formulas\n",
    "If we detect interaction between both continuous and discrete data, we calculate probability as follows:\n",
    "$$P = \\prod_{m=1}^{M}P(Y_{\\{m\\}} \\mid X_{\\{I=h\\}}) P({X_{\\{I=h\\}}}\\mid I)$$\n",
    "\n",
    "where\n",
    "\n",
    "- $M$ is the total number of combination values of $X$ that are associated with $Y$\n",
    "- The formula can be calculated by combining two formulas we defined above\n",
    "\n",
    "#### Parameters\n",
    "- `discTensor`: Input discrete tensor.\n",
    "- `contTensor`: Input continous tensor.\n",
    "\n",
    "#### Returns\n",
    "- `logProb`: Logarithmic likelihood of both continuous and discrete variates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikeDepe(discTensor, contTensor):\n",
    "    combined_tensor = tf.reduce_join(discTensor, axis=0, separator=' ')\n",
    "    unique_tensor, _ = tf.unique(combined_tensor)\n",
    "    \n",
    "    def select_fn(elem):\n",
    "        selected = tf.squeeze(tf.transpose(tf.gather(tf.transpose(contTensor), \n",
    "                    tf.where(tf.equal(combined_tensor, elem)))), [1])\n",
    "        return logLikeCont(selected)\n",
    "    \n",
    "    logProb = tf.map_fn(lambda x: select_fn(x), unique_tensor, dtype=tf.float64)\n",
    "    logProb = tf.reduce_sum(logProb)\n",
    "    logProb += logLikeDisc(discTensor)\n",
    "    \n",
    "    return logProb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function calcProb(tensor1, tensor2)\n",
    "\n",
    "This function is used to calculate likelihood given a dicrete tensor and continuous tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcProb(tensor1, tensor2):\n",
    "    shape1 = tf.shape(tensor1)\n",
    "    shape2 = tf.shape(tensor2)\n",
    "    \n",
    "    pred_fn = OrderedDict([(tf.logical_and(tf.greater(shape1[0], 0), \n",
    "                tf.greater(shape2[0], 0)), lambda: logLikeDepe(tensor1, tensor2)),\n",
    "                (tf.greater(shape1[0], 0), lambda: logLikeDisc(tensor1)),\n",
    "                (tf.greater(shape2[0], 0), lambda: logLikeCont(tensor2))])\n",
    "    res = tf.case(pred_fn, default=lambda: ZERO, exclusive=False)\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset, outFileName, iterNum, burninNum, obsNum, \n",
    "    SNPNum, PhenoNum, MAF) = init()\n",
    "TotalNum = SNPNum + PhenoNum\n",
    "Odds = np.array([(1-MAF)**2, 2*MAF*(1-MAF), MAF**2])\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Define TensorFlow constants.\n",
    "    GeneData = tf.constant(dataset[:SNPNum].astype(np.int32).astype('str'))\n",
    "    PhenoData = tf.constant(dataset[SNPNum:TotalNum])\n",
    "    PI = tf.constant(np.pi, name='PI', dtype=tf.float64)\n",
    "    ZERO = tf.zeros([], dtype=tf.float64)\n",
    "    KAPPA0 = tf.constant(1.0, name='KAPPA', dtype=tf.float64)\n",
    "    NU0 = tf.constant(PhenoNum+1, name='NU', dtype=tf.float64)\n",
    "    MEANS = tf.reduce_mean(PhenoData, axis=1, name='MEANS')\n",
    "    MU0 = tf.reduce_max(MEANS) + 2\n",
    "    \n",
    "    # Define TensorFlow placeholders.\n",
    "    index1 = tf.placeholder(dtype=tf.int32)\n",
    "    index2 = tf.placeholder(dtype=tf.int32)\n",
    "    var1 = tf.placeholder(dtype=tf.int32)\n",
    "    var2 = tf.placeholder(dtype=tf.int32)\n",
    "    \n",
    "    # TensorFlow random number generator.\n",
    "    u = tf.random_uniform([], dtype=tf.float64)\n",
    "    \n",
    "    Dx = index1[:SNPNum]\n",
    "    Cx = index1[SNPNum:TotalNum]\n",
    "    Dy = index2[:SNPNum]\n",
    "    Cy = index2[SNPNum:TotalNum]\n",
    "    \n",
    "    Dxx = tf.squeeze(tf.gather(GeneData, tf.where(tf.equal(Dx, var1))), [1])\n",
    "    Cxx = tf.squeeze(tf.gather(PhenoData, tf.where(tf.equal(Cx, var1))), [1])\n",
    "    Dxy = tf.squeeze(tf.gather(GeneData, tf.where(tf.equal(Dx, var2))), [1])\n",
    "    Cxy = tf.squeeze(tf.gather(PhenoData, tf.where(tf.equal(Cx, var2))), [1])\n",
    "    Dyx = tf.squeeze(tf.gather(GeneData, tf.where(tf.equal(Dy, var1))), [1])\n",
    "    Cyx = tf.squeeze(tf.gather(PhenoData, tf.where(tf.equal(Cy, var1))), [1])\n",
    "    Dyy = tf.squeeze(tf.gather(GeneData, tf.where(tf.equal(Dy, var2))), [1])\n",
    "    Cyy = tf.squeeze(tf.gather(PhenoData, tf.where(tf.equal(Cy, var2))), [1])\n",
    "    \n",
    "    pX = 0 \n",
    "    pY = 0\n",
    "    pX += calcProb(Dxx, Cxx)\n",
    "    pX += calcProb(Dxy, Cxy)\n",
    "    pY += calcProb(Dyx, Cyx)\n",
    "    pY += calcProb(Dyy, Cyy)\n",
    "    \n",
    "    accept = tf.log(u) <= tf.minimum(ZERO, pY-pX)\n",
    "    res = tf.cond(accept, lambda: index2, lambda: index1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 16.67%\n",
      "Progress: 33.33%\n",
      "Progress: 50.00%\n",
      "Progress: 66.67%\n",
      "Progress: 83.33%\n",
      "Progress: 100.00%\n",
      "Training Complete! Index array:\n",
      " [100 100   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  88  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  95  47  48  49  62  51  52  53\n",
      "  54  91  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  92  89\n",
      "  90  91  92  93  94  99  96  97  98  99 100]\n",
      "The whole program runs about 80.26 s.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    start = time.time()\n",
    "    # If you want to use TensorBoard to visualize graph, uncomment the following line.\n",
    "    # writer = tf.summary.FileWriter('output/', sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    Ix = np.arange(TotalNum)\n",
    "    for i in range(iterNum):\n",
    "        while True:\n",
    "            # Sort the number to ensure changing from small index to big one.\n",
    "            x, y = np.sort(np.random.choice(Ix, 2, False))\n",
    "            k = np.where(Ix == x)[0]\n",
    "            \n",
    "            if len(k) > 1:\n",
    "                k = np.random.choice(k, 1)\n",
    "          \n",
    "            Iy = np.array(Ix)\n",
    "            Iy[k] = y\n",
    "          \n",
    "            tmp1 = np.where(Ix == x)[0]\n",
    "            tmp2 = np.where(Iy == y)[0]\n",
    "            if (len(tmp1)!=1 or len(tmp2)!=1):\n",
    "                break\n",
    "        \n",
    "        Ix = sess.run(res, {index1: Ix, index2: Iy, var1: x, var2: y})\n",
    "            \n",
    "        if (i+1) % 5000 == 0:\n",
    "            print('Progress: %.2f%%' % ((i+1)/iterNum*100))\n",
    "                \n",
    "    print('Training Complete! Index array:\\n', Ix)\n",
    "\n",
    "end = time.time()\n",
    "print(\"The whole program runs about %.2f s.\" % (end-start))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
