{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BHIT (Numpy)\n",
    "\n",
    "This notebook uses pure python and **Numpy** package to run BHIT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Packages\n",
    "\n",
    "1. `time` package for measuring running time of the whole program;\n",
    "2. `scipy.speical` package for calculating logrithmic gamma function;\n",
    "3. `numpy` package for matrix manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.special\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Some Helper Functions\n",
    "\n",
    "### Function init()\n",
    "This function intializes parameters that will be used in the program.\n",
    "#### Parameters\n",
    "- `dataset`: The whole dataset read from the input file.\n",
    "- `outFileName`: Name of output file.\n",
    "- `iterNum`: Number of iteration.\n",
    "- `burninNum`: Burn-in number of iterations.\n",
    "- `ObsNum`: Number of observations, e.g., number of population.\n",
    "- `SNPNum`: Number of single nucleotide polymorphisms (SNPs).\n",
    "- `PhenoNum`: Number of phenotype types.\n",
    "- `MAF`: Minor Allele Frequency, should be less than 1.\n",
    "\n",
    "** There are some differences between Jupyter notebook version and command line version! **\n",
    "\n",
    "*We define parameter values inside the function instead of reading them from command line, because Jupyter cannot run with self-defined parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    \n",
    "    dataset = np.loadtxt('input.txt')\n",
    "    outFileName = 'output.npy'\n",
    "    iterNum = 30000\n",
    "    burninNum = 29000\n",
    "    ObsNum = 200\n",
    "    SNPNum = 100\n",
    "    PhenoNum = 1\n",
    "    MAF = 0.5\n",
    "    \n",
    "    return dataset, outFileName, iterNum, burninNum, ObsNum, SNPNum, PhenoNum, MAF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcation unique(arr, return_counts=True)\n",
    "This function finds the unique elements of an array. It returns the sorted unique elements of an array, and the number of times each unique value comes up in the input array if `return_counts` is True.\n",
    "#### Parameters\n",
    "- `arr`: Input array.\n",
    "- `return_counts`: If `True`, also return the number of times each unique item appears in `arr`.\n",
    "\n",
    "#### Returns\n",
    "- `unique`: The sorted unique values.\n",
    "- `unique_counts`: The number of times each of the unique values comes up in the original array.\n",
    "\n",
    "This function is written based on Numpy v1.14 unique() function. Considering that **low version may not support extra parameters**, like `return_counts` we need in this problem, we rewrote the unique() function for our specific usage.\n",
    "\n",
    "*However, if you install Numpy v1.14 on your machine, you may delete this function to see if the program works properly. *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(arr, return_counts=True):\n",
    "    \n",
    "    arr = np.asanyarray(arr)    \n",
    "    orig_shape, orig_dtype = arr.shape, arr.dtype\n",
    "    # Must reshape to a contiguous 2D array for this to work...\n",
    "    arr = arr.reshape(orig_shape[0], -1)\n",
    "    arr = np.ascontiguousarray(arr)\n",
    "\n",
    "    if arr.dtype.char in (np.typecodes['AllInteger'] +\n",
    "                         np.typecodes['Datetime'] + 'S'):\n",
    "        # Optimization: Creating a view of your data with a np.void data type of\n",
    "        # size the number of bytes in a full row. Handles any type where items\n",
    "        # have a unique binary representation, i.e. 0 is only 0, not +0 and -0.\n",
    "        dtype = np.dtype((np.void, arr.dtype.itemsize * arr.shape[1]))\n",
    "    else:\n",
    "        dtype = [('f{i}'.format(i=i), arr.dtype) for i in range(arr.shape[1])]\n",
    "\n",
    "    try:\n",
    "        consolidated = arr.view(dtype)\n",
    "    except TypeError:\n",
    "        # There's no good way to do this for object arrays, etc...\n",
    "        msg = 'The axis argument to unique is not supported for dtype {dt}'\n",
    "        raise TypeError(msg.format(dt=arr.dtype))\n",
    "\n",
    "    def reshape_uniq(uniq):\n",
    "        uniq = uniq.view(orig_dtype)\n",
    "        uniq = uniq.reshape(-1, *orig_shape[1:])\n",
    "        uniq = np.swapaxes(uniq, 0, 0)\n",
    "        return uniq\n",
    "\n",
    "    tmp = np.asanyarray(consolidated).flatten()\n",
    "\n",
    "    if tmp.size == 0:\n",
    "        if not return_counts:\n",
    "            output = tmp\n",
    "        else:\n",
    "            output = (tmp,)\n",
    "            output += (np.empty(0, np.intp),)\n",
    "    else:\n",
    "        tmp.sort()\n",
    "        aux = tmp\n",
    "        flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n",
    "        \n",
    "        if not return_counts:\n",
    "            output = aux[flag]\n",
    "        else:\n",
    "            output = (aux[flag],)\n",
    "            idx = np.concatenate(np.nonzero(flag) + ([tmp.size],))\n",
    "            output += (np.diff(idx),)\n",
    "    \n",
    "    if not return_counts:\n",
    "        return reshape_uniq(output)\n",
    "    else:\n",
    "        uniq = reshape_uniq(output[0])\n",
    "        return (uniq,) + output[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function logLikeCont(contArr)\n",
    "This function calculates logarithmic likelihood of only continuous variates given the data array.\n",
    "#### Math formulas\n",
    "We assume $Gaussian$ distribution on the continuous data, and the probability is:\n",
    "\n",
    "$$P(Y\\mid X) = (\\frac{1}{2\\pi})^n\\sqrt{\\frac{\\kappa_0}{\\kappa_n}}\\frac{\\Gamma(v_n/2)}{\\Gamma(v_0/2)}((\\frac{v_0\\sigma_0^2}{2})^{v_0/2}/(\\frac{v_n\\sigma_n^2}{2})^{v_n/2})$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\kappa_n = \\kappa_0 + n$\n",
    "- $v_n = v_0 + n$\n",
    "- $\\sigma_n^2 = \\frac{1}{v_n}(v_0\\sigma_0^2+(n-1)s^2+\\frac{\\kappa_0 n}{\\kappa_n}(\\bar y - \\mu_0)^2)$\n",
    "- $s^2 = \\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar y)^2$\n",
    "- $\\kappa_0, \\mu_0, \\sigma_0, v_0$ is pre-defined in the program ([click here](#Run-This-Program)), $n$ is the observation number of continuous data\n",
    "\n",
    "#### Parameters\n",
    "- `contArr`: Input array.\n",
    "\n",
    "#### Returns\n",
    "- `logProb`: Logarithmic likelihood of continuous variates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikeCont(contArr):\n",
    "    \n",
    "    Y = np.asanyarray(contArr)\n",
    "    obsNum, varNum = Y.shape\n",
    "    logProb = 0\n",
    "    \n",
    "    if varNum == 0:\n",
    "        pass\n",
    "    elif varNum == 1:\n",
    "        sigma = 1\n",
    "        mean = np.average(Y)\n",
    "        nuVar = NU0 * sigma**2\n",
    "        nuVar += (obsNum-1) * np.var(Y)\n",
    "        nuVar += KAPPA0*obsNum/(KAPPA0+obsNum) * (mean-MU0)**2\n",
    "\n",
    "        logProb = -1*np.log(2*np.pi)*obsNum/2 + np.log(KAPPA0/(KAPPA0 + obsNum)) / 2 + scipy.special.gammaln((NU0+obsNum)/2)\n",
    "        logProb += (-1*scipy.special.gammaln(NU0/2) + np.log(NU0*sigma**2/2)*NU0/2 - np.log(nuVar/2) * (NU0+obsNum)/2)\n",
    "    \n",
    "    # The below code was not fully tested.\n",
    "    else:\n",
    "        means = np.average(Y, axis=0)\n",
    "        lambda_arr = np.diag([1]*varNum)\n",
    "        diff = np.array(means - MU0)[:,None]\n",
    "        lambdaN = (lambda_arr + KAPPA0 * obsNum / (KAPPA0+obsNum)\n",
    "                    * diff.dot(diff.transpose()))\n",
    "        lambdaN += (obsNum-1)*np.cov(Y, rowvar=False, bias=False)\n",
    "\n",
    "        logProb = (-np.log(np.pi) * obsNum * varNum / 2 + np.log(KAPPA0/\n",
    "            (KAPPA0 + obsNum) * varNum / 2))\n",
    "        logProb += np.log(np.linalg.det(lambda_arr)) * NU0/2\n",
    "        logProb -= np.log(np.linalg.det(lambdaN)) * (NU0+obsNum)/2\n",
    "        logProb += np.sum(scipy.special.gammaln((NU0+obsNum)/2 - \n",
    "            np.arange(varNum)/2) - scipy.special.gammaln(NU0/2 - \n",
    "            np.arange(varNum)/2))\n",
    "        \n",
    "    return logProb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function logLikeDisc(discArr)\n",
    "This function calculates logarithmic likelihood of only discrete variates given the data array.\n",
    "#### Math formulas\n",
    "We assume $Dirichlet$ distribution on the discrete data, and the probability is:\n",
    "\n",
    "$$P(p_1, \\cdots, p_{C_h}\\mid \\alpha_1, \\cdots, \\alpha_{C_h}) = \\frac{1}{B(\\alpha)}\\prod_{j=1}^{C_h}p_{j}^{\\alpha_{j-1}} $$\n",
    "\n",
    "where \n",
    "\n",
    "- $C_h$ is the possible combination values in the genetic variation group (incoming parameter `discArr`)\n",
    "- $\\alpha = (\\alpha_1, \\cdots, \\alpha_{C_h})$, parameter vector in $Dirichlet$ distribution\n",
    "- $B(\\alpha) = \\frac{\\prod_{j=1}^{C_h}\\Gamma(\\alpha_j)}{\\Gamma(\\sum_{j=1}^{C_h}\\alpha_j)}$\n",
    "\n",
    "\n",
    "By integrating these, we can get following formula:\n",
    "$$ P(X\\mid I) = \\prod_{j=1}^{C_h}\\frac{\\Gamma(n_j+\\alpha_j)}{\\Gamma(\\alpha_j)}\\frac{\\Gamma(\\sum_{j=1}^{C_h}\\alpha_j)}{\\Gamma(\\sum_{j=1}^{C_h}(n_j+\\alpha_j))}$$\n",
    "\n",
    "where \n",
    "\n",
    "- $X$ is the genetic variation group, `discArr` in the program\n",
    "- $I$ is current partition, `Ix` or `Iy` in the program\n",
    "- $n_j$ denotes the number of j-th value in possible combination shown up in the genetic variation group $X$\n",
    "- $\\Gamma (x) = \\int_0^{\\infty}t^{x-1}e^{-t}dt$.\n",
    "\n",
    "#### Parameters:\n",
    "- `discArr`: Input array.\n",
    "\n",
    "#### Returns\n",
    "- `logProb`: Logarithmic likelihood of discrete variates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikeDisc(discArr):\n",
    "\n",
    "    X = np.asanyarray(discArr)\n",
    "    uniqueArr, N = unique(X)\n",
    "    \n",
    "    alpha = odds[uniqueArr-1]\n",
    "    alpha = np.prod(alpha, axis=1)\n",
    "    n_plus_alpha = N + alpha\n",
    "    \n",
    "    logProb = np.sum(scipy.special.gammaln(n_plus_alpha) - scipy.special.gammaln(alpha))\n",
    "    logProb -= scipy.special.gammaln(np.sum(n_plus_alpha))\n",
    "    return logProb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function logLikeDepe(discArr, contArr)\n",
    "This function calculates logarithmic likelihood of partitions with both continuous and discrete variates.\n",
    "#### Math formulas\n",
    "If we detect interaction between both continuous and discrete data, we calculate probability as follows:\n",
    "$$P = \\prod_{m=1}^{M}P(Y_{\\{m\\}} \\mid X_{\\{I=h\\}}) P({X_{\\{I=h\\}}}\\mid I)$$\n",
    "\n",
    "where\n",
    "\n",
    "- $M$ is the total number of combination values of $X$ that are associated with $Y$\n",
    "- The formula can be calculated by combining two formulas we defined above\n",
    "\n",
    "#### Parameters\n",
    "- `discArr`: Input discrete array.\n",
    "- `contArr`: Input continous array.\n",
    "#### Returns\n",
    "- `logProb`: Logarithmic likelihood of both continuous and discrete variates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikeDepe(discArr, contArr):\n",
    "    \n",
    "    X = np.asanyarray(discArr)\n",
    "    Y = np.asanyarray(contArr)\n",
    "    variations = unique(X, return_counts=False)\n",
    "    logProb = 0\n",
    "    \n",
    "    for v in variations:\n",
    "        corres_row = np.prod((X==v), axis=1)\n",
    "        corres_Y = Y[corres_row==1]\n",
    "        logProb += logLikeCont(corres_Y)\n",
    "    logProb += logLikeDisc(X)\n",
    "\n",
    "    return logProb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function metroHast(iterNum, burninNum)\n",
    "\n",
    "The function uses Metropolis–Hastings algorithm for sampling, and runs as follows:\n",
    "1. Initialize index vector to be $[1, \\cdots, n]$, where $n$ is the sum of SNP number and phenotype number;\n",
    "2. Select one index from the vector and change it to another one randomly;\n",
    "3. Calculate likelihood of original index partition vector, i.e., `old_prob` in the program;\n",
    "4. Calculate likelihood of new index partition vector, i.e., `new_prob` in the program;\n",
    "5. If the later likelihood is greater than the former, we update the index partition vector and other relevant staff (actually we might also update under some situations).\n",
    "\n",
    "#### Parameters\n",
    "- `iterNum`: Number of iteration of MCMC.\n",
    "- `burninNum`: Number of burn-in of MCMC.\n",
    "\n",
    "#### Returns\n",
    "- `mhResult`: Final partition matrix for each covariate.\n",
    "- `Ix`: Final index vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metroHast(iterNum, burninNum):\n",
    "    \n",
    "    Ix = np.arange(TotalNum)\n",
    "    Dx = Ix[:SNPNum]\n",
    "    Cx = Ix[SNPNum:TotalNum]\n",
    "    iN = np.zeros([TotalNum, TotalNum+1])\n",
    "    \n",
    "    # Uncomment if you want to trace probabilities.\n",
    "    # trace = np.zeros(iterNum)\n",
    "    # trace[0] += np.sum([logLikDisc(genotype[:, Dx==col]) for col in range(SNPNum)])\n",
    "    # trace[0] += np.sum([logLikCont(phenotype[:, Cx==col]) for col in range(SNPNum,TotalNum)])\n",
    "\n",
    "    # Main Metropolis-Hastings loop.\n",
    "    for i in range(1, iterNum):\n",
    "        # Select an index, then change it to another index randomly.\n",
    "        while True:\n",
    "            # Sort the number to ensure changing from small index to big one.\n",
    "            x, y = np.sort(np.random.choice(Ix, 2, False))\n",
    "            k = np.where(Ix == x)[0]\n",
    "            \n",
    "            if len(k) > 1:\n",
    "                k = np.random.choice(k, 1)\n",
    "          \n",
    "            Iy = np.array(Ix)\n",
    "            Iy[k] = y\n",
    "          \n",
    "            tmp1 = np.where(Ix == x)[0]\n",
    "            tmp2 = np.where(Iy == y)[0]\n",
    "            if (len(tmp1)!=1 or len(tmp2)!=1):\n",
    "                break\n",
    "\n",
    "        # Create the proposed indicator vector.\n",
    "        Dy = Iy[:SNPNum]\n",
    "        Cy = Iy[SNPNum:TotalNum]\n",
    "        Cxx = phenotype[:,Cx == x]\n",
    "        Cxy = phenotype[:,Cx == y]\n",
    "        Cyx = phenotype[:,Cy == x]\n",
    "        Cyy = phenotype[:,Cy == y]\n",
    "        Dxx = genotype[:,Dx == x]\n",
    "        Dxy = genotype[:,Dx == y]\n",
    "        Dyx = genotype[:,Dy == x]\n",
    "        Dyy = genotype[:,Dy == y]\n",
    "        \n",
    "        # Calculate log likelihoods.\n",
    "        old_prob = 0\n",
    "        new_prob = 0\n",
    "        \n",
    "        # Likelihood of current partition x.\n",
    "        if Cxx.size != 0:\n",
    "            if Dxx.size != 0:\n",
    "                old_prob += logLikeDepe(Dxx, Cxx)\n",
    "            else:\n",
    "                old_prob += logLikeCont(Cxx)\n",
    "        elif Dxx.size != 0:\n",
    "            old_prob += logLikeDisc(Dxx)\n",
    "        \n",
    "        # Likelihood of current partition x.\n",
    "        if Cxy.size != 0:\n",
    "            if Dxy.size != 0:\n",
    "                old_prob += logLikeDepe(Dxy, Cxy)\n",
    "            else:\n",
    "                old_prob += logLikeCont(Cxy)\n",
    "        elif Dxy.size != 0:\n",
    "            old_prob += logLikeDisc(Dxy)\n",
    "        \n",
    "        # Likelihood of proposed partition y.\n",
    "        if Cyx.size != 0:\n",
    "            if Dyx.size != 0:\n",
    "                new_prob += logLikeDepe(Dyx, Cyx)\n",
    "            else:\n",
    "                new_prob += logLikeCont(Cyx)\n",
    "        elif Dyx.size != 0:\n",
    "            new_prob += logLikeDisc(Dyx)\n",
    "\n",
    "        # Likelihood of proposed partition y.\n",
    "        if Cyy.size != 0:\n",
    "            if Dyy.size != 0:\n",
    "                new_prob += logLikeDepe(Dyy, Cyy)\n",
    "            else:\n",
    "                new_prob += logLikeCont(Cyy)\n",
    "        elif Dyy.size != 0:\n",
    "            new_prob += logLikeDisc(Dyy)\n",
    "        \n",
    "        # Uncomment if you want to trace probabilities.\n",
    "        # trace[i] = trace[i-1]\n",
    "        \n",
    "        # Check if proposal is accepted, if so, update everything.\n",
    "        accept = np.log(np.random.rand()) <= min(0.0, new_prob-old_prob)\n",
    "        if accept:\n",
    "            Ix = np.array(Iy)\n",
    "            Cx = np.array(Cy)\n",
    "            Dx = np.array(Dy)\n",
    "            # trace[i] += new_prob-old_prob\n",
    "        \n",
    "        if (i+1) % 5000 == 0:\n",
    "            print(\"Progress: %.2f%%\" % ((i+1)/iterNum*100))\n",
    "            \n",
    "        # When MCMC gets convergence, we start to count indices.\n",
    "        if i >= burninNum:\n",
    "            for h in range(TotalNum):\n",
    "                tmp = np.where(Ix==h)[0]\n",
    "                if (len(tmp) == 1):\n",
    "                    iN[tmp, 0] += 1\n",
    "                elif (len(tmp) > 1):\n",
    "                    iN[tmp, h+1] += 1\n",
    "\n",
    "    # Normalize rows of result to show proportions.\n",
    "    iN = iN/np.sum(iN, axis=1)[:,None]\n",
    "    cst = np.sum(iN, axis=0)[1:]\n",
    "\n",
    "    # Remove columns that have no values in them.\n",
    "    tmp = np.where(cst != 0)[0]\n",
    "    iNt = iN[:, 1:]\n",
    "    if len(tmp) == 0:\n",
    "        iNtr = iNt[:,1]\n",
    "    else:\n",
    "        iNtr = iNt[:,tmp]\n",
    "    \n",
    "    # Have the first column correspond to all independent variates, \n",
    "    # and others are partitions with more than one variate.\n",
    "    mhResult = np.zeros([TotalNum+1, len(tmp)+2])\n",
    "    mhResult[0, 1:] = np.arange(len(tmp)+1)\n",
    "    mhResult[1:SNPNum+1, 0] = np.arange(1, SNPNum+1)\n",
    "    mhResult[SNPNum+1:, 0] = np.arange(1, PhenoNum+1)\n",
    "    mhResult[1:,1] = iN[:TotalNum,0]\n",
    "    mhResult[1:,2:] = iNtr\n",
    "    \n",
    "    # Uncomment if you want to save trace file.\n",
    "    # np.savetxt(outFileName+\"_trace\", trace, fmt='%.1f')\n",
    "    \n",
    "    return mhResult, Ix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run This Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization Completed!\n",
      "Progress: 16.67%\n",
      "Progress: 33.33%\n",
      "Progress: 50.00%\n",
      "Progress: 66.67%\n",
      "Progress: 83.33%\n",
      "Progress: 100.00%\n",
      "The whole program runs about 42.86s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Initialize parameters for later usage.\n",
    "(dataset, outFileName, iterNum, burninNum, \n",
    " ObsNum, SNPNum, PhenoNum, MAF) = init()\n",
    "TotalNum = SNPNum + PhenoNum\n",
    "genotype = dataset[:, :SNPNum].astype(int)\n",
    "phenotype = dataset[:, SNPNum:TotalNum]\n",
    "odds = np.array([(1-MAF)**2, 2*MAF*(1-MAF), MAF**2])\n",
    "\n",
    "# Define hyper-parameters here.\n",
    "KAPPA0 = 1\n",
    "NU0 = PhenoNum + 1\n",
    "MEANS = np.average(phenotype, axis=0)\n",
    "MU0 = max(MEANS) + 2\n",
    "\n",
    "# Check if input file format is valid.\n",
    "if (genotype.shape[0] != phenotype.shape[0]):\n",
    "    print(\"Discrete and continuous data must have same number of rows!\\n\")\n",
    "    quit()\n",
    "print(\"Initialization Completed!\")\n",
    "\n",
    "# Detection using Metropolis–Hastings algorithm.\n",
    "mhResult, index = metroHast(iterNum, burninNum)\n",
    "np.savetxt(outFileName, mhResult, fmt='%i')\n",
    "\n",
    "# Output running time of the program.\n",
    "end = time.time()\n",
    "print(\"The whole program runs about %.2fs.\" % (end-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The function `metroHast` actually returns two values: one is the final index vector, and the other is the independence array of each variate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   1.   2.]\n",
      " [  1.   0.   0.   1.]\n",
      " [  2.   0.   0.   1.]\n",
      " [  3.   1.   0.   0.]\n",
      " [  4.   1.   0.   0.]\n",
      " [  5.   1.   0.   0.]\n",
      " [  6.   1.   0.   0.]\n",
      " [  7.   1.   0.   0.]\n",
      " [  8.   1.   0.   0.]\n",
      " [  9.   1.   0.   0.]\n",
      " [ 10.   1.   0.   0.]\n",
      " [ 11.   1.   0.   0.]\n",
      " [ 12.   1.   0.   0.]\n",
      " [ 13.   1.   0.   0.]\n",
      " [ 14.   1.   0.   0.]\n",
      " [ 15.   1.   0.   0.]\n",
      " [ 16.   1.   0.   0.]\n",
      " [ 17.   1.   0.   0.]\n",
      " [ 18.   1.   0.   0.]\n",
      " [ 19.   1.   0.   0.]\n",
      " [ 20.   1.   0.   0.]\n",
      " [ 21.   1.   0.   0.]\n",
      " [ 22.   1.   0.   0.]\n",
      " [ 23.   1.   0.   0.]\n",
      " [ 24.   1.   0.   0.]\n",
      " [ 25.   1.   0.   0.]\n",
      " [ 26.   1.   0.   0.]\n",
      " [ 27.   1.   0.   0.]\n",
      " [ 28.   1.   0.   0.]\n",
      " [ 29.   1.   0.   0.]\n",
      " [ 30.   1.   0.   0.]\n",
      " [ 31.   1.   0.   0.]\n",
      " [ 32.   1.   0.   0.]\n",
      " [ 33.   1.   0.   0.]\n",
      " [ 34.   1.   0.   0.]\n",
      " [ 35.   1.   0.   0.]\n",
      " [ 36.   1.   0.   0.]\n",
      " [ 37.   1.   0.   0.]\n",
      " [ 38.   1.   0.   0.]\n",
      " [ 39.   1.   0.   0.]\n",
      " [ 40.   1.   0.   0.]\n",
      " [ 41.   1.   0.   0.]\n",
      " [ 42.   1.   0.   0.]\n",
      " [ 43.   1.   0.   0.]\n",
      " [ 44.   1.   0.   0.]\n",
      " [ 45.   1.   0.   0.]\n",
      " [ 46.   1.   0.   0.]\n",
      " [ 47.   1.   0.   0.]\n",
      " [ 48.   1.   0.   0.]\n",
      " [ 49.   1.   0.   0.]\n",
      " [ 50.   1.   0.   0.]\n",
      " [ 51.   1.   0.   0.]\n",
      " [ 52.   1.   0.   0.]\n",
      " [ 53.   1.   0.   0.]\n",
      " [ 54.   1.   0.   0.]\n",
      " [ 55.   1.   0.   0.]\n",
      " [ 56.   1.   0.   0.]\n",
      " [ 57.   1.   0.   0.]\n",
      " [ 58.   1.   0.   0.]\n",
      " [ 59.   1.   0.   0.]\n",
      " [ 60.   1.   0.   0.]\n",
      " [ 61.   1.   0.   0.]\n",
      " [ 62.   1.   0.   0.]\n",
      " [ 63.   1.   0.   0.]\n",
      " [ 64.   1.   0.   0.]\n",
      " [ 65.   1.   0.   0.]\n",
      " [ 66.   1.   0.   0.]\n",
      " [ 67.   1.   0.   0.]\n",
      " [ 68.   1.   0.   0.]\n",
      " [ 69.   1.   0.   0.]\n",
      " [ 70.   1.   0.   0.]\n",
      " [ 71.   1.   0.   0.]\n",
      " [ 72.   1.   0.   0.]\n",
      " [ 73.   1.   0.   0.]\n",
      " [ 74.   1.   0.   0.]\n",
      " [ 75.   1.   0.   0.]\n",
      " [ 76.   1.   0.   0.]\n",
      " [ 77.   1.   0.   0.]\n",
      " [ 78.   1.   0.   0.]\n",
      " [ 79.   1.   0.   0.]\n",
      " [ 80.   1.   0.   0.]\n",
      " [ 81.   1.   0.   0.]\n",
      " [ 82.   1.   0.   0.]\n",
      " [ 83.   1.   0.   0.]\n",
      " [ 84.   1.   0.   0.]\n",
      " [ 85.   1.   0.   0.]\n",
      " [ 86.   1.   0.   0.]\n",
      " [ 87.   1.   0.   0.]\n",
      " [ 88.   1.   0.   0.]\n",
      " [ 89.   1.   0.   0.]\n",
      " [ 90.   1.   0.   0.]\n",
      " [ 91.   0.   1.   0.]\n",
      " [ 92.   1.   0.   0.]\n",
      " [ 93.   1.   0.   0.]\n",
      " [ 94.   1.   0.   0.]\n",
      " [ 95.   1.   0.   0.]\n",
      " [ 96.   1.   0.   0.]\n",
      " [ 97.   1.   0.   0.]\n",
      " [ 98.   0.   1.   0.]\n",
      " [ 99.   1.   0.   0.]\n",
      " [100.   1.   0.   0.]\n",
      " [  1.   0.   0.   1.]]\n"
     ]
    }
   ],
   "source": [
    "print(mhResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row of `mhResult` is an indicator row, which tells us how many interactions detected in the dataset. In this example it is 2. The second column tells independent variates in the dataset. As we only care about the relation between genotype and phenotype, the above result is accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 100   2   3   4   5   6   7   8   9  39  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  90  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  97  91  92  93  94  95  96  97  98  99 100]\n"
     ]
    }
   ],
   "source": [
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from final index vector, there are some indices changed during the process. For example, both the first and second value are changed to 100."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
